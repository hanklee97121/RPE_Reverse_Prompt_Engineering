import openai
import os
import re
import spacy
import collections
import numpy as np
import copy
import time
import evaluate
from tqdm.notebook import trange, tqdm
import pandas as pd
import json
openai.api_key = "YOUR OPENAI API KEY"
os.environ["OPENAI_API_KEY"] = "YOUR OPENAI API KEY"

class Reverse_Prompt_Engineering():
    def __init__(self, X=1, N=1, model="gpt-3.5-turbo", p=0.5, tag_model="en_core_web_sm"):
        '''
        Initialize the optimization process
        parameter:
          X: number of iteration for optimization
          N: number of candidate prompts to keep at each optimization step
          p: probability to add words from answers to children (mutation probability) #not using
          tag_model: #not using
        '''
        self.X = X
        self.N = N
        self.p = p
        self.llm = openai.OpenAI()
        self.answers = []
        self.model = model
        self.candidate_prompts = []
        self.candidate_scores = []
        self.candidate_total_scores = []
        self.candidate_answers = []
        self.parent_candidates = []
        self.tag_model = spacy.load(tag_model)
        self.ignore_tags = set(['X', 'PUNCT','SPACE', 'PART', 'AUX', 'PROPN'])
        self.ori_prompt = None
        self.answers_word_set = None
        self.rouge = evaluate.load('rouge')

    def reset(self, X=1, N=1, p=0.5):
        self.X = X
        self.N = N
        self.p = p
        self.answers = []
        
        self.candidate_prompts = []
        self.candidate_scores = []
        self.candidate_total_scores = []
        self.candidate_answers = []
        self.parent_candidates = []
        self.answers_word_set = None
        
    def __call__(self, ini_prompts=None, ini_scores=None, ini_total_scores=None, ini_answers=None):
        """
        Recover prompt from answers
        """
        #generate prompt based on answers
        self.initial_prompt_generation(ini_prompts)
        #evaluation
        if ini_scores and ini_total_scores and ini_answers:
            self.candidate_scores, self.candidate_total_scores, self.candidate_answers = ini_scores, ini_total_scores, ini_answers
        else:
            self.candidate_scores, self.candidate_total_scores, self.candidate_answers = self.evaluation(self.candidate_prompts)

        
        for i in range(self.X):
            #crossover
            children_candidates = self.cross_mutation()

            #evaluation
            
            children_scores, children_total_scores, children_answers = self.evaluation(children_candidates)

            #replacement
            self.replacement(children_candidates, children_scores, children_answers, children_total_scores)

        idx = np.argmax(self.candidate_scores)
        final_prompt = self.candidate_prompts[idx]
        return final_prompt
        


    def chat(self, messages, model="gpt-3.5-turbo", n=1, t=0.5):
        """
        define chat function
        """
        

        response = self.llm.chat.completions.create(
            model=model,
            messages=messages,
            n=n,
            temperature=t
        )
        text_response = [response.choices[i].message.content for i in range(n)]

        
        return text_response

    def generate_answers(self, prompt=None, answers:list=[], n:int=1, t=0.5):
        """
        Generate answers we want to infer from. Create tag dict for all answers
        parameter:
          prompt: the original prompt we use to generate answers
          answers: optional, if provided, just copy this list to self.answers
          n: number of answers to generate for searching original prompt
        """
        if answers:
            self.answers = answers
        else:
            message = [{"role":"user", "content":prompt}]
            self.answers = self.chat(message, model = self.model, n=n, t=t)
        
        self.ori_prompt = prompt
        

    def initial_prompt_generation(self, ini_prompts=None):
        if ini_prompts:
            for p in ini_prompts:
                self.candidate_prompts.append(p)
        else:
            prompt_0 = "All answers above are generated by one prmopt passing through LLM multiple times. Based on the answers provided above, speculate the underlying prompt."
            message_0 = [{"role":"system", "content":"As a smart dectactive, you can infer the underlying prompt besed on outputs of language model."}] + \
                    [{"role":"user", "content":"Answer "+str(i)+": "+self.answers[i]} for i in range(len(self.answers))] + \
                    [{"role":"user", "content":prompt_0}] + \
                    [{"role":"system", "content":"Think step by step and return the prompt beginning with <PRO> and end with </PRO>."}]
            prompt_candidates = self.chat(message_0, n=self.N, t=1)
            for prompt in prompt_candidates:
                splited_prompt = re.split('<PRO>|</PRO>', prompt)
                #if the format is not correct, regenerate the prompt until the format is correct
                while len(splited_prompt) != 3:
                    prompt = self.chat(message_0, n=1)[0]
                    splited_prompt = re.split('<PRO>|</PRO>', prompt)
                self.candidate_prompts.append(splited_prompt[1])

    def evaluation(self, prompt_list = []):
        """
        Evaluate each pompt candidate in prompt_list
        """
        total_score_list = []
        final_score_list = []
        candidate_answers_list = []
        n = len(prompt_list)
        m = len(self.answers)
        for j in range(n):
            
            message_prompt = [{"role":"user", "content":prompt_list[j]}]
            answer_e = self.chat(message_prompt, t=0)[0]
            score_list = []
            
            for i in range(m):
                
                predictions = [answer_e]
                references = [self.answers[i]]
                results = self.rouge.compute(predictions=predictions,
                          references=references)
                score = results["rouge1"]
                score_list.append(float(score))
                
                
               
                
            final_score = (np.mean(score_list)+max(score_list))/2
            #final_score = np.mean(score_list)
            #final_score = max(score_list)
            total_score_list.append(score_list)
            final_score_list.append(final_score)
            candidate_answers_list.append(answer_e)
            
        return final_score_list, total_score_list, candidate_answers_list


    def pos_tagging(self, text_list):
        """
        Tag each word in sentences of text_list, remove stop word and unuseful word, return a set
        """
        
        token_list = []
        word_set = set([])
        for text in text_list:
            text_seg = self.tag_model(str(text))
            for chunk in text_seg.noun_chunks:
                if chunk.root.dep_ == "appos":
                    continue
                for token in chunk:
                    if token.is_stop:
                        continue
                    elif token.pos_ in self.ignore_tags:
                        continue
                    elif not token.is_alpha:
                        continue
                    word_set.add(token.lemma_)
        
        return word_set


    def cross_mutation(self):
        '''
        For each candidate prompt, get difference between candidate answer and original answers
        summarize that
        ask llm to give advise of revising prompt
        do the revise
        '''
        children_candidates = []
        
        for i in range(5):
            old_prompt = self.candidate_prompts[i]
            old_answer = self.candidate_answers[i]
            m = len(self.answers)
            suggest_list = []
            #difference between answers
            for i in range(m):
                prompt_e = "You should answer concisely with no redundant information. Given two answers below, how is the candidate answer different from "+\
                "the reference answer? If there is no significant difference, you can answer 'There is no difference'."
                message_e = [{"role":"user", "content": prompt_e}]+\
                            [{"role":"user", "content": "Candidate Answer: "+old_answer}]+\
                            [{"role":"user", "content": "Reference Answer: "+self.answers[i]}]
                suggest_e = self.chat(message_e)
                suggest_list.append(suggest_e[0])

            #summarize difference
            prompt_s = "Given a list of responses, summarize them into one concise response."
            message_s = [{"role":"user", "content": prompt_s}] + \
                        [{"role":"user", "content": "List of responses: " + str(suggest_list)}]
            summary_s = self.chat(message_s)[0]

            #generate advice to revise prompt
            prompt_p = "According to the difference above, how should you change the prompt that "+\
                        "generate the candidate answer to make the candidate answer more similar to the reference answer? Answer concisely."
            message_p = [{"role":"user", "content": "Difference: " + summary_s}] + \
                        [{"role":"user", "content": prompt_p}] 
            difference_p = self.chat(message_p)[0]

            #revise the prompt
            prompt_r = "Using the suggestion above to revise the following prompt." 
            message_r = [{"role":"user", "content": "Suggestion: " + difference_p}] + \
                        [{"role":"user", "content": prompt_r}] + \
                        [{"role":"user", "content": "Prompt: " + old_prompt}] + \
                        [{"role":"user", "content": "The prompt should be concise. Return the prompt beginning with <PRO> and end with </PRO>."}]
            child_r = self.chat(message_r)[0]

            splited_child = re.split('<PRO>|</PRO>', child_r)
            while len(splited_child) != 3:
                child_r = self.chat(message_r)[0]
                splited_child = re.split('<PRO>|</PRO>', child_r)
            children_candidates.append(splited_child[1])
            
        return children_candidates
        
    def replacement(self, children_prompts, children_scores, children_answers, children_total_scores):
        for i in range(5):
            child_score = children_scores[i]
            if child_score > min(self.candidate_scores):
                idx = np.argmin(self.candidate_scores)
                self.candidate_prompts[idx] = children_prompts[i]
                self.candidate_answers[idx] = children_answers[i]
                self.candidate_total_scores[idx] = children_total_scores[i]
                self.candidate_scores[idx] = child_score
